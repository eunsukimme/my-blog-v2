---
title: Linear Regression(ì„ í˜• íšŒê·€) ê°œë… ì •ë¦¬
summary: Supervised Learningì—ì„œ ì—°ì†ì ì¸ ê°’ì„ ì˜ˆì¸¡í•˜ëŠ” Linear Regressionì— ëŒ€í•´ì„œ ì•Œì•„ë´…ì‹œë‹¤
date: '2019-10-15T00:00:00.000Z'
draft: false
slug: 'understanding-linear-regression'
category: Machine Learning
tags:
  - TIL
  - Machine Learning
  - Python
  - scikit-learn
---

ML ì„ ê³µë¶€í•˜ëŠ” ëª©ì ì€ í˜„ì‹¤ ì„¸ê³„ì˜ ë°ì´í„°ë¥¼ í•´ì„í•˜ì—¬ ë‹¤ìŒë²ˆì— ì–´ë–¤ ì¼ì´ ë°œìƒí• ì§€ ì˜ˆì¸¡í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤. ì´ ë•Œ í™œìš©í•  ìˆ˜ ìˆëŠ” ê°€ì¥ ê°„ë‹¨í•œ ëª¨ë¸ì€ ë°”ë¡œ ì§ì„ ì…ë‹ˆë‹¤. ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ëŒ€í‘œí•˜ëŠ” í•˜ë‚˜ì˜ ì§ì„ ì„ ì°¾ëŠ” ê²ƒì´ ë°”ë¡œ **Linear Regression** ì…ë‹ˆë‹¤.

ë‹¤ìŒê³¼ ê°™ì´ ì•¼êµ¬ ì„ ìˆ˜ì˜ í‚¤ì— ë”°ë¥¸ ëª¸ë¬´ê²Œ ë°ì´í„°ê°€ ìˆë‹¤ê³  ìƒê°í•´ë´…ì‹œë‹¤.

<img
  width="400"
  src="https://s3.amazonaws.com/codecademy-content/programs/data-science-path/linear_regression/weight_height.png"
/>

ìœ„ ë°ì´í„°ë¥¼ ê°€ë¡œì§€ë¥´ëŠ” ê°€ì¥ 'ì ì ˆí•œ' ì§ì„ ì„ ê·¸ë¦¬ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

<img
  width="400"
  src="https://s3.amazonaws.com/codecademy-content/programs/data-science-path/linear_regression/weight_height_line.png"
/>

ì´ ë•Œ í‚¤ê°€ 73 ì¸ì¹˜ì¸ ìƒˆë¡œìš´ ì„ ìˆ˜ê°€ ë“¤ì–´ì˜¬ ë•Œ ëª¸ë¬´ê²Œê°€ ëŒ€ëµ 143 íŒŒìš´ë“œì— ê°€ê¹Œìš¸ ê²ƒì´ë¼ëŠ” ê²ƒì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¬¼ë¡  ì´ëŸ¬í•œ ì§ì„ ì€ ëŸ¬í”„í•œ ê·¼ì‚¬ì¹˜ë¥¼ ì œê³µí•  ë¿ì´ì§€ë§Œ, ì„ í˜•ì ì¸ ê´€ê³„ê°€ ìˆëŠ” ë°ì´í„°ì— ëŒ€í•´ì„œëŠ” ë‹¤ìŒì— ë‚˜íƒ€ë‚  ë°ì´í„°ë¥¼ ì˜ˆì¸¡í•˜ëŠ”ë°ì— ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

# Points and Lines

í•˜ë‚˜ì˜ ì§ì„ ì€ *slope*ì™€ *intercept*ì— ì˜í•´ ì •ì˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$y = mx + b$$

ì—¬ê¸°ì„œ `m` ì´ slope ê³ , `b` ê°€ intercept ì…ë‹ˆë‹¤. Linera Regression ì„ ìˆ˜í–‰í•  ë–„, ìµœì¢… ëª©í‘œëŠ” ë°”ë¡œ ë°ì´í„°ë¥¼ ê°€ì¥ 'ì˜' ë‚˜íƒ€ë‚´ëŠ” `m` ê³¼ `b` ë¥¼ ì°¾ëŠ” ê²ƒì…ë‹ˆë‹¤. ê°€ì¥ 'ì˜' ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ì˜ë¯¸ëŠ” ë°”ë¡œ ë‹¤ìŒ ì„¹ì…˜ì—ì„œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤.

# Loss

ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ ë‚˜íƒ€ë‚´ëŠ” slope ì™€ intercept ë¥¼ ì°¾ê¸° ìœ„í•´ì„  ê°€ì¥ 'ì˜' ë‚˜íƒ€ë‚¸ë‹¤ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ ì •ì˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤.

ê°ê°ì˜ ë°ì´í„° í¬ì¸íŠ¸ì— ëŒ€í•´ì„œ, ìš°ë¦¬ëŠ” model ì´ ì˜ˆì¸¡í•œ ê°’ì´ ì–¼ë§ˆë‚˜ ì˜ ëª» ë˜ì—ˆëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” **loss** ë¼ëŠ” ê²ƒì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ loss ëŠ” ì ê³¼ model ì´ ì œê³µí•œ ì§ì„  ì‚¬ì´ì˜ ê±°ë¦¬ì˜ ì œê³±ìœ¼ë¡œ ìƒê°í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì œê³±ì„ í•´ì£¼ëŠ” ì´ìœ ëŠ” ë¶€í˜¸ê°€ ë‹¬ë¼ì„œ ê±°ë¦¬ì˜ í•©ì´ ìƒì‡„ë˜ëŠ” ê²ƒì„ ì˜ˆë°©í•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.

ì´ë ‡ê²Œ ê°ê°ì˜ ì ê³¼ ì§ì„  ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ê³„ì‚°í•´ì„œ í‰ê· ì„ ë‚¸ ê°’ì„ ìµœì†Œí™” í•œë‹¤ë©´, ê·¸ ë•Œ ê·¸ë ¤ì§€ëŠ” ì§ì„ ì€ ë°ì´í„°ë¥¼ ê°€ë¡œì§€ë¥´ëŠ” ì§ì„ ë“¤ ì¤‘ ë°ì´í„°ë¥¼ ê°€ì¥ 'ì˜' ë‚˜íƒ€ëƒˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ë°ì´í„°ì™€ ì§ì„ ì´ ìˆë‹¤ê³  ìƒê°í•´ë´…ì‹œë‹¤.

<img
  width="700"
  alt="Screen Shot 2019-10-15 at 2 41 24 AM"
  src="https://user-images.githubusercontent.com/31213226/66771498-59a69b80-eef5-11e9-9f52-3e79752a560e.png"
/>

í˜„ì¬ slope ê°€ -0.04, intercept ê°€ -0.02 ì¸ ì§ì„ ì˜ loss ëŠ” 32.12 ì…ë‹ˆë‹¤. ëˆˆìœ¼ë¡œë§Œ ë´ë„ ìœ„ ì§ì„ ì€ ë°ì´í„°ë¥¼ ì˜ ë‚˜íƒ€ë‚´ê³  ìˆë‹¤ê³  ë§í•˜ê¸° ì–´ë ¤ìš¸ ê²ƒ ê°™ìŠµë‹ˆë‹¤.

<img
  width="700"
  alt="Screen Shot 2019-10-15 at 2 43 52 AM"
  src="https://user-images.githubusercontent.com/31213226/66771651-d0439900-eef5-11e9-9b35-e081be6c9fbb.png"
/>

ìœ„ ì§ì„ ì€ slope ê°€ -0.75, intercept ê°€ 4.62 ì¸ë° loss ê°€ 2.39 ë¡œ ì „ë³´ë‹¤ ë” ë‚®ì•„ì§„ê±¸ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë¦¼ìƒìœ¼ë¡œë„ ë°ì´í„°ë¥¼ ì˜ ë‚˜íƒ€ë‚´ê³  ìˆëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤.

ì‚¬ëŒì´ ì§ì ‘ ì§ì„ ì˜ slope ì™€ intercept ë¥¼ ì¡°ì •í•  ìˆ˜ ìˆì§€ë§Œ, ì´ëŠ” ê³¼í•™ì ì´ì§€ë„ ì•Šê³  ì‹œê°„ë„ ì˜¤ë˜ ê±¸ë¦½ë‹ˆë‹¤. ì–´ë–»ê²Œ í•˜ë©´ ê³¼í•™ì ìœ¼ë¡œ ìµœì ì˜ `m` ê³¼ `b` ë¥¼ ì°¾ì„ ìˆ˜ ìˆì„ê¹Œìš”?

# Gradient Descent

ìµœì ì˜ `m` ê³¼ `b` ëŠ” loss ê°€ ê°€ì¥ í¬ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ `m` ê³¼ `b` ë¥¼ ì´ë™ì‹œí‚¤ëŠ” **gradient Descent** ë¼ëŠ” ë°©ë²•ì„ í†µí•´ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ gradient ëŠ” íŠ¹ì • ì§€ì ì—ì„œì˜ ê¸°ìš¸ê¸°(ë¯¸ë¶„)ì„ ëœ»í•©ë‹ˆë‹¤.

ë¨¼ì € loss ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ì •ì˜í•œ ë’¤, loss ë¥¼ ê° íŒŒë¼ë¯¸í„° `m` ê³¼ `b` ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ ê°’ì„ ê³„ì‚°í•¨ìœ¼ë¡œì¨ loss ê°€ ê°ì†Œì‹œí‚¤ëŠ” íŒŒë¼ë¯¸í„°ì˜ ë°©í–¥ì„ êµ¬í•œ ë’¤ `m` ê³¼ `b` ë¥¼ ì—…ë°ì´íŠ¸ ì‹œì¼œì£¼ë©´ ë©ë‹ˆë‹¤.

## Gradient descent for Intercept

í•˜ë‚˜ì˜ ë°ì´í„°ì— ëŒ€í•œ gradient ë¥¼ êµ¬í•˜ê³  ì´ë¥¼ ì „ì²´ ë°ì´í„°ì— ëŒ€í•´ ì¼ë°˜í™” í•˜ë„ë¡ í•©ì‹œë‹¤. ë¨¼ì € ë°ì´í„°ë¥¼ ëŒ€í‘œí•  ì§ì„  a ì„ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ë–„ \i ê¸°í˜¸ëŠ” í•˜ë‚˜ì˜ ë°ì´í„° í¬ì¸íŠ¸, ì¦‰ training example ì„ ëœ»í•©ë‹ˆë‹¤.

$$ a_i = mx_i + b$$

ê·¸ëŸ¬ë©´ ê° training example ì— ëŒ€í•œ loss $L$ ì„ ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$$L_i = (y_i - a_i)^2$$

ê·¸ë¦¬ê³  ëª¨ë“  N ê°œì˜ training example ì— ëŒ€í•œ loss ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê° loss ì˜ ì´ í•©ì„ êµ¬í•œ ë’¤ N ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì£¼ë©´ ë©ë‹ˆë‹¤.

$$L = {1 \over N } \displaystyle\sum_{i=1}^n(y_i - a_i)^2$$

ì´ì œ loss ë¥¼ intercept ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ gradient $dL \over db$ë¥¼ ê³„ì‚°í•´ë´…ì‹œë‹¤. ê·¸ëŸ¬ê¸° ìœ„í•´ì„  ë¨¼ì € $L$ë¥¼ $a$ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ $dL \over da$ ì„ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.

$$ {dL \over da} = {2 \over N} \displaystyle\sum\_{i=1}^n (a_i - y_i)$$

ê·¸ë¦¬ê³  $a$ë¥¼ $b$ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ gradient $da \over db$ ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.

$${da \over db} = 1$$

ê·¸ëŸ¬ë©´ ì´ì œ gradient chain rule ì— ë”°ë¼ $dL \over db$ ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$${dL \over db} = {da \over db} * {dL \over da} = {2 \over N} \displaystyle\sum_{i=1}^n (a_i - y_i)$$

ì´ë ‡ê²Œ í•´ì„œ loss ë¥¼ intercept ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ gradient ë¥¼ ê³„ì‚°í•˜ì˜€ìŠµë‹ˆë‹¤. $a_i$ë¥¼ ë§¨ ì²˜ìŒì— ì •ì˜í•œ ìˆ˜ì‹ìœ¼ë¡œ ëŒ€ì…í•˜ê³  ìˆœì„œë¥¼ ì¡°ê¸ˆ ë³€ê²½í•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì€ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$${dL \over db} = {2 \over N} \displaystyle\sum_{i=1}^N -(y_i - (mx_i + b))$$

## Gradient descent for slope

ë§ˆì°¬ê°€ì§€ë¡œ loss ë¥¼ slope ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ gradient $dL \over dm$ ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. $da \over dm$ ë§Œ ê³„ì‚°í•˜ë©´ ìœ„ì—ì„œ êµ¬í•œ ê°’ë“¤ì„ ê·¸ëŒ€ë¡œ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

${da \over dm} = x_i$

ì¦‰ gradient chain rule ì— ë”°ë¼ $dL \over dm$ ë¥¼ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$${dL \over dm} = {da \over dm} _ {dL \over da} = {2 \over N} \displaystyle\sum_{i=1}^n x_i * (a_i - y_i)$$

ì´ë ‡ê²Œ í•´ì„œ loss ë¥¼ slope ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•œ gradient ë¥¼ ê³„ì‚°í•˜ì˜€ìŠµë‹ˆë‹¤. $a_i$ ì— ì²˜ìŒ ìˆ˜ì‹ì„ ëŒ€ì„í•˜ê³  ìˆœì„œë¥¼ ì¡°ê¸ˆ ë³€ê²½í•˜ë©´ ìµœì¢…ì ìœ¼ë¡œ ë‹¤ìŒ ìˆ˜ì‹ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

$${dL \over dm} = {2 \over N} \displaystyle\sum_{i=1}^N -x_i * (y_i - (mx_i + b))$$

## Python code

ìœ„ ìˆ˜ì‹ì„ ê·¸ëŒ€ë¡œ ì½”ë“œë¡œ ì˜®ê¸°ë©´ ë°”ë¡œ gradient descent ë¥¼ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¼ì €, intercept ì˜ gradient ë¥¼ ê³„ì‚°í•˜ëŠ” `get_gradient_at_b()` í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ì‹œë‹¤. ì´ í•¨ìˆ˜ëŠ” íŒŒë¼ë¯¸í„°ë¡œ x-axis ê°’ ë°°ì—´ `x`, y-axis ê°’ ë°°ì—´ `y`, slope `m`, intercept `b` ë¥¼ ë°›ìŠµë‹ˆë‹¤.

```python
def get_gradient_at_b(x, y, m, b):
  diff = sum([y_i - (m * x_i + b) for x_i, y_i in zip(x, y)])
  b_gradient = diff * (-2 / len(x))

  return b_gradient
```

ë˜ slope ì˜ gradient ë¥¼ ê³„ì‚°í•˜ëŠ” `get_gradient_at_m()` í•¨ìˆ˜ë¥¼ ì‘ì„±í•©ì‹œë‹¤. ì´ í•¨ìˆ˜ë„ `get_gradient_at_b()` ì™€ ë™ì¼í•œ íŒŒë¼ë¯¸í„°ë¥¼ ë°›ìŠµë‹ˆë‹¤.

```python
def get_gradient_at_m(x, y, m, b):
  diff = sum([x_i * (y_i - (m * x_i + b)) for x_i, y_i in zip(x, y)])
  m_gradient = diff * (-2 / len(x))

  return m_gradient
```

ì´ì œ ê³„ì‚°í•œ gradient ë¥¼ í†µí•´ `b` ì™€ `m` ì„ ì—…ë°ì´íŠ¸ í•´ì£¼ëŠ” í•¨ìˆ˜ `step_gradient()` ë¥¼ ì‘ì„±í•´ë´…ì‹œë‹¤. ì´ í•¨ìˆ˜ëŠ” loss ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ í•œ step ë‚˜ì•„ê°”ì„ ë•Œ ì—…ë°ì´íŠ¸ ëœ ìƒˆë¡œìš´ `b` ì™€ `m` ì„ ë°˜í™˜í•´ì¤ë‹ˆë‹¤. ì´ ë•Œ, í•œ step ì˜ í¬ê¸°ê°€ ë„ˆë¬´ í¬ê±°ë‚˜ ì‘ìœ¼ë©´ ë¬¸ì œê°€ë˜ë‹ˆ ì´ë¥¼ ìŠ¤ì¼€ì¼ë§ í•  ìˆ˜ ìˆë„ë¡ learning_rate ë¼ëŠ” í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ê°€ë¡œ ë°›ì•„ ê° gradient ì™€ ê³±í•´ì¤ì‹œë‹¤.

```python
def step_gradient(x, y, m_current, b_current, learning_rate):
  b_gradient = get_gradient_at_b(x, y, m_current, b_current)
  m_gradient = get_gradient_at_m(x, y, m_current, b_current)
  b = b_current - (learning_rate * b_gradient)
  m = m_current - (learning_rate * m_gradient)

  return [b, m]
```

ì, ì´ì œ gradient ë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì„ ì•Œì•˜ìœ¼ë‹ˆ, ì‹¤ì œë¡œ ì˜ ì‘ë™í•˜ëŠ” ê±´ì§€ í…ŒìŠ¤íŠ¸ë¥¼ í•´ë´…ì‹œë‹¤. ë¨¼ì € ì˜ˆë¥¼ ë“¤ì–´, ì›”ë³„ ìˆ˜ìµ(ë‹¬ëŸ¬)ì„ ë‚˜íƒ€ë‚´ëŠ” ë°ì´í„°ê°€ ìˆë‹¤ê³  ìƒê°í•´ë´…ì‹œë‹¤.

```python
from matplotlib import pyplot as plt

months = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]
revenue = [52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184]
# ì›”ë³„ ìˆ˜ìµ ê·¸ë˜í”„ë¥¼ ì‚°ì ë„ë¡œ ë‚˜íƒ€ë‚¸ë‹¤
plt.plot(months, revenue, 'o')
plt.show()
```

<img
  width="400"
  alt="Screen Shot 2019-10-15 at 3 56 52 AM"
  src="https://user-images.githubusercontent.com/31213226/66775797-d9396800-eeff-11e9-910d-c7aa03e28bb5.png"
/>

ë¨¼ì € `b` ì™€ `m` ëª¨ë‘ 0 ìœ¼ë¡œ ì´ˆê¸°í™” ì‹œí‚¤ê³ , loss ë¥¼ ê°ì†Œì‹œí‚¤ëŠ” ë°©í–¥ìœ¼ë¡œ í•œ step ë‚˜ì•„ê°”ì„ ë–„ ì–´ë–»ê²Œ ë³€í™”í•˜ëŠ”ì§€ í™•ì¸í•´ ë´…ì‹œë‹¤. ì²˜ìŒ ê·¸ë¦¬ëŠ” ì§ì„ ì´ ì˜¤ëœì§€ìƒ‰, í•œ step ì´í›„ì— ê·¸ë¦¬ëŠ” ì§ì„ ì´ ì´ˆë¡ìƒ‰ ì…ë‹ˆë‹¤.

```python
# bì™€ mì„ 0ìœ¼ë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤
b1, m1 = 0, 0
y1 = [m1 * month + b1 for month in months]
plt.plot(months, y1)
# learning_rateë¥¼ 0.001ë¡œ ì„¤ì •í•œ í›„ í•œ step ë‚˜ì•„ê°‘ë‹ˆë‹¤
b2, m2 = step_gradient(months, revenue, b1, m1, 0.001)
y2 = [m2 * month + b2 for month in months]
plt.plot(months, y2)

plt.show()
```

<img
  width="400"
  alt="Screen Shot 2019-10-15 at 4 17 05 AM"
  src="https://user-images.githubusercontent.com/31213226/66776973-b066a200-ef02-11e9-9c10-6cda88c80483.png"
/>

ì´ˆë¡ìƒ‰ ì§ì„ ì´ ì‚´ì§ ìœ„ë¡œ ì˜¬ë¼ê°„ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤! ëª‡ step ì„ ë” ê±°ì¹˜ë©´ ë°ì´í„°ë¥¼ ì˜ í‘œí˜„í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° ì´ step ì€ ì–¼ë§ˆë‚˜ ê°€ì•¼í•  ì§€ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œìš”?

# Convergence

íŒŒë¼ë¯¸í„° `m` ê³¼ `b` ë¥¼ ì–¸ì œê¹Œì§€ ë³€í™”ì‹œì¼œì¤˜ì•¼ í• ê¹Œìš”? ì–¼ë§ˆë‚˜ í•™ìŠµì„ ì‹œì¼œì•¼ ì¶©ë¶„íˆ í•™ìŠµí–ˆë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ê¹Œìš”?

ì´ë¥¼ ìœ„í•´ì„  ë°˜ë“œì‹œ convergence ë¥¼ ì•Œì•„ì•¼í•©ë‹ˆë‹¤. **convergence** ë€ íŒŒë¼ë¯¸í„°ê°€ ë³€í™”í•˜ë”ë¼ë„ loss ê°€ ë”ì´ìƒ ì˜ ë³€í™”í•˜ì§€ ì•ŠëŠ” í…€ì„ ë§í•©ë‹ˆë‹¤. ì•„ë˜ ê·¸ë˜í”„ëŠ” ì•½ 800 iterations ë¶€í„° ê±°ì˜ ë³€í™”ê°€ ì—†ëŠ”ë°, ì´ì‹œê¸°ê°€ ë°”ë¡œ convergence ì…ë‹ˆë‹¤. ì¦‰, ì•„ë˜ì˜ ê²½ìš° 800 iterations ì •ë„ë©´ ì¶©ë¶„íˆ í•™ìŠµí–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

<img
  width="400"
  alt="Screen Shot 2019-10-15 at 4 25 41 AM"
  src="https://user-images.githubusercontent.com/31213226/66777382-de98b180-ef03-11e9-8a38-af35d54e4db4.png"
/>

# Learning rate

ë°ì´í„°ë¥¼ ê°€ì¥ 'ì˜' ë‚˜íƒ€ë‚´ëŠ” ì§ì„ ì„ ì°¾ê¸° ìœ„í•´ì„  `m` ê³¼ `b` ë¥¼ loss ê°€ ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ë‚˜ì•„ê°€ê²Œ í•´ì•¼ í•œë‹¤ê³  ì•ì—ì„œ ì„¤ëª…í•˜ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ°ë° 'ì–¼ë§ˆë‚˜' ë‚˜ì•„ê°€ì•¼ ì ì ˆí•˜ê²Œ ë‚˜ì•„ê°€ëŠ” ê±¸ê¹Œìš”?

ê·¸ë˜ì„œ ìš°ë¦¬ëŠ” **learning rate** ë¥¼ ì„ íƒí•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, í•œ step ì—ì„œ gradient ê°€ ë³€í™”í•˜ëŠ” ì–‘ì„ ì •í•´ì£¼ì–´ì•¼ í•©ë‹ˆë‹¤. ë„ˆë¬´ ì‘ì€ learning rate ëŠ” converge í•˜ê¸°ê¹Œì§€ ë§ì€ ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. ë„ˆë¬´ í° learning rate ëŠ” ê°€ì¥ ì ì€ loss ê°’ì„ ì§€ë‚˜ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜¤íˆë ¤ ë°œì‚°í•˜ê²Œ ë˜ëŠ” ê²½ìš°ê°€ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

ì ˆëŒ€ì ìœ¼ë¡œ ëª¨ë“  ëª¨ë¸ì— ì í•©í•œ learning rate ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¤ë§Œ í•„ìš”í•œ ë§Œí¼ìœ¼ë¡œ íš¨ìœ¨ì ì´ê³  converge í•˜ê²Œ ë˜ëŠ” ì¶©ë¶„íˆ í° learning rate ë¥¼ ì°¾ì•„ì•¼ í•©ë‹ˆë‹¤. ë„ˆë¬´ ê±±ì •í•˜ì§„ ë§ˆì„¸ìš”. ì¼ë°˜ì ìœ¼ë¡œ ì ì ˆí•˜ë‹¤ê³  íŒë‹¨ëœëŠ” learning rate ë“¤ ëª‡ ê°œë§Œ ì ìš©í•´ë³´ë©´ ê¸ˆë°© ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤ :)

# Put it Together

ì´ì œ ìš°ë¦¬ì˜ model ì„ ë§ˆì € í•™ìŠµì‹œì¼œ ë´…ì‹œë‹¤. í•„ìš”í•œ ê²ƒì€ ëª¨ë‘ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ í•˜ë‚˜ì˜ step ë§Œ ë‚˜ì•„ê°€ëŠ” í•¨ìˆ˜ ë°–ì— ì—†ìœ¼ë‹ˆ, ì£¼ì–´ì§„ num_iterations ë§Œí¼ step ì„ ë°˜ë³µí•˜ëŠ” `gradient_descent()` í•¨ìˆ˜ë¥¼ ë§Œë“­ì‹œë‹¤.

```python
def gradient_descent(x, y, learning_rate, num_iterations):
  m, b = 0, 0	# 0ìœ¼ë¡œ ì´ˆê¸°í™”
  for i in range(num_iterations):
    # num_iterations ë§Œí¼ stepì„ ì§„í–‰í•©ë‹ˆë‹¤
    b, m = step_gradient(b, m, x, y, learning_rate)

  return [b, m]
```

ì´ì œ learning_rate ëŠ” 0.01, num_iteration ì€ 1000 ìœ¼ë¡œ í•™ìŠµì„ ì§„í–‰í•´ë´…ì‹œë‹¤.

```python
b, m = gradient_descent(months, revenue, 0.01, 1000)
y = [m*x + b for x in months]

plt.plot(months, revenue, "o")
plt.plot(months, y)
plt.show()
```

<img
  width="400"
  alt="Screen Shot 2019-10-15 at 4 42 37 AM"
  src="https://user-images.githubusercontent.com/31213226/66778311-3afcd080-ef06-11e9-9e19-f583a1813ab9.png"
/>

ì§ì„ ì´ ë°ì´í„°ë¥¼ ì•„ì£¼ ì˜ ëŒ€í‘œí•˜ëŠ” ë“¯ í•©ë‹ˆë‹¤! í•™ìŠµì´ ì˜ ì´ë¤„ì§„ ê²ƒ ê°™ìŠµë‹ˆë‹¤ ğŸ‘

# Scikit-Learn

ì§€ê¸ˆê¹Œì§€ êµ¬í˜„í•œ ëª¨ë“  í•¨ìˆ˜ë¥¼ ë§¤ë²ˆ regression í•  ë•Œë§ˆë‹¤ ë‹¤ì‹œ ì‘ì„±í•´ì•¼ í•œë‹¤ë©´ ë„ˆë¬´ ì†ê°€ë½ì´ ì•„í”„ê² ì£ ? ë‹¤í–‰íˆë„, Scikit-Learn ì´ë¼ëŠ” Python ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ML ê³¼ ê´€ë ¨ëœ ëª¨ë“ˆë“¤ì„ ì œê³µí•˜ê³  ìˆìŠµë‹ˆë‹¤. Linear regression ì˜ ê²½ìš° scikit-learn ì˜ `linear_model` ëª¨ë“ˆì—ì„œ `LinearRegression()` í•¨ìˆ˜ë¥¼ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
from sklearn.linear_model import LinearRegression
```

ê·¸ë¦¬í•˜ì—¬ Linear Regression ëª¨ë¸ì„ ìƒì„±í•œ ë’¤ ë°ì´í„°ë¥¼ fitting í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
line_fitter = LinearRegression()
line_fitter.fit(X, y)
```

`.fit()` í•¨ìˆ˜ëŠ” ëª¨ë¸ì—ê²Œ ë‹¤ìŒ ë‘ ë³€ìˆ˜ë¥¼ ì œê³µí•©ë‹ˆë‹¤. ì´ëŠ” ìš°ë¦¬ê°€ ë‚˜ì¤‘ì— ìœ ìš©í•˜ê²Œ ì ‘ê·¼í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

1. `line_fitter.coef_` - slope ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.
2. `line_fitter.intercept_` - intercept ë¥¼ ì €ì¥í•©ë‹ˆë‹¤.

ë˜í•œ ìš°ë¦¬ëŠ” `.predict()` í•¨ìˆ˜ì— x-values ë¥¼ ì „ë‹¬í•˜ì—¬ ì˜ˆì¸¡ëœ y-values ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

```python
y_predicted = line_fitter.predict(X)
```

**Note**: `num_iterations` ì™€ `learning_rate` ëŠ” scikit-learn ì—ì„œ dafault value ë¡œ ê°€ì§€ê³  ìˆìœ¼ë‹ˆ ì´ì— ëŒ€í•´ì„  ë”°ë¡œ ëª…ì‹œí•´ ì£¼ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤.

ì, ê·¸ëŸ¼ ìš°ë¦¬ê°€ ì—´ì‹¬íˆ ì§  ì½”ë“œë¥¼ scikit-learn ìœ¼ë¡œ implement í•´ë´…ì‹œë‹¤.

```python
from sklearn.linear_model import LinearRegression
import matplotlib.pyplot as plt
import numpy as np

months = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])
months = months.reshape(-1, 1)
revenue = np.array([52, 74, 79, 95, 115, 110, 129, 126, 147, 146, 156, 184])

line_fitter = LinearRegression()
line_fitter.fit(months, revenue)
revenue_predict = line_fitter.predict(months)

plt.plot(months, revenue, 'o')
plt.plot(months, revenue_predict)
plt.show()
```

<img
  width="400"
  alt="Screen Shot 2019-10-15 at 4 54 11 AM"
  src="https://user-images.githubusercontent.com/31213226/66778869-d9d5fc80-ef07-11e9-9651-518f98a37853.png"
/>

ì½”ë“œê°€ í›¨ì”¬ ê°„ê²°í•´ ì§„ ê²ƒê³¼ ë™ì¼í•œ ê²°ê³¼ë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤ ğŸ˜š

# Review

ì§€ê¸ˆê¹Œì§€ linear Regression ì„ Python ê³¼ scikit-learn ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ í™œìš©í•˜ì—¬ ì–´ë–»ê²Œ êµ¬í˜„í•˜ëŠ”ì§€ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.

- We can measure how well a line fits by measuring _loss_.
- The goal of linear regression is to minimize loss.
- To find the line of best fit, we try to find the `b` value (intercept) and the `m` value (slope) that minimize loss.
- _Convergence_ refers to when the parameters stop changing with each iteration.
- _Learning rate_ refers to how much the parameters are changed on each iteration.
- We can use Scikit-learnâ€™s `LinearRegression()` model to perform linear regression on a set of points.

ì´ì œ ë°°ìš´ ë‚´ìš©ì„ ì¨ë¨¹ìœ¼ëŸ¬ ê°€ë´ì•¼ê² ì£ ? Kaggle ì´ë‚˜ scikit-learn ì—ì„œ ì œê³µí•˜ëŠ” ì˜ˆì‹œ ë°ì´í„°ë¥¼ ê°€ì§€ê³  í•œë²ˆ ë†€ì•„ë³´ëŠ” ì‹œê°„ì„ ê°€ì ¸ë´…ì‹œë‹¤ ğŸ˜‹

# References

- [Codecademy](http://www.codecademy.com)
